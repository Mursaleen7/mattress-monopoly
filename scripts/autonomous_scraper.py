#!/usr/bin/env python3
"""
‚ö†Ô∏è  DEPRECATED - USE run_scraper.py INSTEAD ‚ö†Ô∏è

This file has been refactored into a modular structure in scripts/scraper/
See scripts/README_REFACTOR.md for migration guide.

CRITICAL SECURITY ISSUE: This file contains hardcoded API keys.
Please migrate to the new version which uses environment variables.

AUTONOMOUS MULTI-AGENT ORCHESTRATION PIPELINE
Generates 100% of v5.0 "God Mode" schema with zero human intervention

Architecture:
- Phase 1: Foundation Layer (APIs for deterministic data)
- Phase 2: Reconnaissance Layer (Search & fetch official sources)
- Phase 3: Intelligence Layer (LLM structured extraction)
- Phase 4: Charisma Synthesis Layer (LLM copywriting)
- Phase 5: Competitor Triangulation (Price scraping)
- Phase 6: Cross-Verification & Confidence Scoring
"""

import warnings
warnings.warn(
    "‚ö†Ô∏è  This script is deprecated and has security issues (hardcoded API keys). "
    "Please use 'python run_scraper.py' instead. See scripts/README_REFACTOR.md",
    DeprecationWarning,
    stacklevel=2
)

import os
import json
import asyncio
import random
import time
from typing import Dict, List, Optional, Tuple
from datetime import datetime
import re

# Core dependencies
import requests
from bs4 import BeautifulSoup
import google.generativeai as genai
from pydantic import BaseModel

# API clients
GEMINI_API_KEY = os.getenv('GEMINI_API_KEY', 'AIzaSyBOFqBJ1TeWC56RgLxPy8_FaKbfmmql7EQ')
GOOGLE_MAPS_API_KEY = os.getenv('GOOGLE_MAPS_API_KEY', 'AIzaSyC0julW4pIMfdBobnzotEFFb4pLyW6osFI')
SERPAPI_KEY = os.getenv('SERPAPI_KEY', 'f0c944b02f8007a33e645a1a519f6689cecdda54ee60c22dc9d38fa5233c5b79')

# Configure Gemini
genai.configure(api_key=GEMINI_API_KEY)

# Configuration
USER_AGENTS = [
    'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',
    'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36',
    'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36'
]

# City knowledge base ‚Äî deterministic facts that never change
CITY_KNOWN_DATA = {
    'Austin': {
        'official_phone': '3-1-1 (512-974-2000)',
        'department_name': 'Austin Resource Recovery',
        'website_url': 'https://www.austintexas.gov/department/austin-resource-recovery',
        'illegal_dumping_fine': 'Up to $2,000 (Class B misdemeanor)',
        'illegal_dumping_citation': 'Austin City Code Chapter 12-8',
    },
    'Dallas': {
        'official_phone': '3-1-1 (214-670-3111)',
        'department_name': 'Dallas Sanitation Services',
        'website_url': 'https://dallascityhall.com/departments/sanitation',
        'illegal_dumping_fine': '$500-$2,000',
        'illegal_dumping_citation': 'Dallas City Code Chapter 18',
    },
    'Houston': {
        'official_phone': '3-1-1 (713-837-0311)',
        'department_name': 'Houston Solid Waste Management',
        'website_url': 'https://www.houstontx.gov/solidwaste',
        'illegal_dumping_fine': '$500-$4,000 (Class A/B misdemeanor)',
        'illegal_dumping_citation': 'Texas Health & Safety Code ¬ß365.012',
    },
}


class CityDataSchema(BaseModel):
    """Pydantic schema enforcing v5.0 structure"""
    city_slug: str
    city_name: str
    state_name: str


# CORRECTED schemas for Gemini - Using dict-based approach for nested structures
# Gemini's structured output has issues with nested Pydantic models

class ContactSchema(BaseModel):
    """Agent 1: Contacts extraction"""
    official_phone: str | None = None
    department_name: str | None = None
    website_url: str | None = None


class FacilitySchema(BaseModel):
    """Single drop-off facility"""
    name: str
    address: str | None = None
    type: str | None = None
    hours: str | None = None
    tipping_fee: str | None = None
    residency_required: bool | None = None
    notes: str | None = None


class NavigatorSchema(BaseModel):
    """Agent 3: Facilities extraction"""
    drop_off_locations: list[FacilitySchema] = []


class CharismaSchema(BaseModel):
    """Phase 4: Copywriting"""
    hero_hook: str
    seo_title: str
    seo_description: str
    neighborhoods: str



    state_slug: str
    state_abbr: str
    geo: Dict
    seo: Optional[Dict] = None
    hero_hook: Optional[str] = None
    neighborhoods: Optional[str] = None
    population: Dict
    contacts: Dict
    curbside_rules: Dict
    weather_profile: Dict
    drop_off_locations: List[Dict]
    affiliate_config: Optional[Dict] = None
    donation_policy: Optional[str] = None
    illegal_dumping: Dict
    audit_metadata: Dict


class AutonomousScraper:
    """
    Multi-agent orchestration system for autonomous city data generation
    Using Gemini with PROPERLY FIXED structured output
    """
    
    def __init__(self):
        self.session = requests.Session()
        self.gemini_model = genai.GenerativeModel('gemini-2.5-flash')
        self.verification_log = []
        
    async def scrape_city_autonomous(self, city_name: str, state_abbr: str, state_name: str) -> Dict:
        """
        Main orchestration method - generates 100% of v5.0 schema
        """
        # Reset log for each city (prevents cross-contamination)
        self.verification_log = []
        
        print(f"\n{'='*80}")
        print(f"ü§ñ AUTONOMOUS PIPELINE: {city_name}, {state_abbr}")
        print(f"{'='*80}\n")
        
        # Phase 1: Foundation Layer (Deterministic APIs)
        print("üìç PHASE 1: Foundation Layer (Geo + Census + Weather)")
        geo_data = await self._phase1_foundation(city_name, state_abbr)
        
        # Phase 2: Reconnaissance Layer (Search & Fetch)
        print("\nüîç PHASE 2: Reconnaissance Layer (Official Sources)")
        official_content = await self._phase2_reconnaissance(city_name, state_abbr)
        
        # Phase 3: Intelligence Layer (LLM Extraction)
        print("\nüß† PHASE 3: Intelligence Layer (Structured Extraction)")
        extracted_data = await self._phase3_intelligence(official_content, city_name, state_abbr)
        
        # Phase 4: Charisma Synthesis Layer (Copywriting)
        print("\n‚ú® PHASE 4: Charisma Synthesis (SEO + Hero + Neighborhoods)")
        charisma_data = await self._phase4_charisma(extracted_data, geo_data, city_name, state_abbr)
        
        # Phase 5: Competitor Triangulation
        print("\nüí∞ PHASE 5: Competitor Triangulation (Pricing)")
        competitor_data = await self._phase5_competitor(city_name, state_abbr)
        
        # Phase 6: Assembly & Validation
        print("\nüîß PHASE 6: Assembly & Validation")
        final_data = self._phase6_assembly(
            city_name, state_name, state_abbr,
            geo_data, extracted_data, charisma_data, competitor_data
        )
        
        return final_data
    
    async def _phase1_foundation(self, city_name: str, state_abbr: str) -> Dict:
        """
        Phase 1: Get deterministic data from official APIs
        - Google Geocoding API: lat/lng, ZIP codes
        - Census API: Population
        - Weather data: Rainfall averages
        """
        foundation = {}
        
        # 1A: Google Geocoding API
        try:
            geocode_url = f"https://maps.googleapis.com/maps/api/geocode/json"
            params = {
                'address': f"{city_name}, {state_abbr}",
                'key': GOOGLE_MAPS_API_KEY
            }
            response = requests.get(geocode_url, params=params, timeout=10)
            data = response.json()
            
            if data['status'] == 'OK':
                location = data['results'][0]['geometry']['location']
                foundation['latitude'] = location['lat']
                foundation['longitude'] = location['lng']
                
                # Extract ZIP codes from address components
                zip_codes = []
                for result in data['results'][:5]:
                    for component in result.get('address_components', []):
                        if 'postal_code' in component['types']:
                            zip_codes.append(component['long_name'])
                
                foundation['zip_codes'] = list(set(zip_codes))[:10]
                
                self._log('geo_api', 'SUCCESS', f"Lat: {location['lat']}, Lng: {location['lng']}")
            else:
                self._log('geo_api', 'FAILED', data['status'])
                foundation['latitude'] = None
                foundation['longitude'] = None
                foundation['zip_codes'] = []
        except Exception as e:
            self._log('geo_api', 'ERROR', str(e))
            foundation['latitude'] = None
            foundation['longitude'] = None
            foundation['zip_codes'] = []
        
        # 1B: Census API (Population)
        try:
            # Use Census QuickFacts URL pattern
            census_url = f"https://www.census.gov/quickfacts/fact/table/{city_name.lower().replace(' ', '')}city{state_abbr.lower()}"
            foundation['population'] = {
                'count': None,  # Would need actual API call
                'year': 2020,
                'source': census_url,
                '_needs_api_call': True
            }
            self._log('census_api', 'QUEUED', census_url)
        except Exception as e:
            self._log('census_api', 'ERROR', str(e))
        
        # 1C: Weather Profile (NOAA data)
        rainy_cities = {
            'Seattle': True, 'Portland': True, 'Miami': True,
            'New Orleans': True, 'Houston': True, 'Mobile': True,
            'Pensacola': True, 'Baton Rouge': True, 'Tallahassee': True,
            'Jacksonville': True, 'Birmingham': True, 'Memphis': True,
            'Nashville': True, 'Louisville': True, 'Cincinnati': True
        }
        
        is_rainy = rainy_cities.get(city_name, False)
        foundation['weather_profile'] = {
            'is_rain_heavy': is_rainy,
            'rejection_risk_copy': f"WARNING: {city_name} averages 100+ rainy days per year. If your mattress gets wet before pickup, the city WILL NOT take it (too heavy, mold risk). Our haulers pick up from inside your home - rain or shine." if is_rainy else None
        }
        
        self._log('weather_profile', 'DETERMINED', f"Rainy: {is_rainy}")
        
        return foundation
    
    async def _phase2_reconnaissance(self, city_name: str, state_abbr: str) -> Dict:
        """
        Phase 2: Search and fetch official government sources + PDFs
        Enhanced with PDF extraction for 40% more data coverage
        """
        content = {
            'gov_pages': [],
            'pdf_text': [],
            'relevant_chunks': []  # Filtered chunks for extraction
        }
        
        # 2A: Find official .gov waste management pages
        try:
            if SERPAPI_KEY:
                # Use SerpApi for precise .gov site search - be very specific
                serp_url = "https://serpapi.com/search"
                params = {
                    'q': f'site:.gov "{city_name}" waste management mattress disposal -site:epa.gov',
                    'api_key': SERPAPI_KEY,
                    'num': 5
                }
                response = requests.get(serp_url, params=params, timeout=15)
                results = response.json()
                
                for result in results.get('organic_results', [])[:3]:
                    url = result['link']
                    
                    # Skip if URL doesn't contain the city name (with Dallas exception)
                    is_city_domain = (
                        city_name.lower() in url.lower() or
                        url.startswith('https://dallas.gov') or
                        url.startswith('https://www.dallas.gov') or
                        url.startswith('https://dallascityhall.com')
                    )
                    if not is_city_domain:
                        self._log('gov_page_scraped', 'SKIPPED', f"{url} - doesn't match {city_name}")
                        continue
                    
                    try:
                        # Check if it's a PDF
                        if url.lower().endswith('.pdf'):
                            pdf_text = self._extract_pdf(url)
                            if pdf_text:
                                content['pdf_text'].append({
                                    'url': url,
                                    'text': pdf_text[:20000]
                                })
                                self._log('pdf_extracted', 'SUCCESS', url)
                        else:
                            # Regular HTML page
                            page_response = self.session.get(url, timeout=10, headers={
                                'User-Agent': random.choice(USER_AGENTS)
                            })
                            soup = BeautifulSoup(page_response.content, 'html.parser')
                            
                            # Extract PDF links from the page
                            for link in soup.find_all('a', href=True):
                                href = link['href']
                                if '.pdf' in href.lower() and any(kw in href.lower() for kw in ['waste', 'bulk', 'guide', 'trash', 'mattress']):
                                    pdf_url = href if href.startswith('http') else url.rstrip('/') + '/' + href.lstrip('/')
                                    pdf_text = self._extract_pdf(pdf_url)
                                    if pdf_text:
                                        content['pdf_text'].append({
                                            'url': pdf_url,
                                            'text': pdf_text[:20000]
                                        })
                                        self._log('pdf_extracted', 'SUCCESS', pdf_url)
                            
                            # Remove non-content
                            for tag in soup(['script', 'style', 'nav', 'footer', 'header']):
                                tag.decompose()
                            
                            text = soup.get_text(separator='\n', strip=True)
                            content['gov_pages'].append({
                                'url': url,
                                'text': text[:15000]  # Limit to 15k chars
                            })
                            
                            self._log('gov_page_scraped', 'SUCCESS', url)
                        time.sleep(random.uniform(1, 3))  # Jitter
                    except Exception as e:
                        self._log('gov_page_scraped', 'FAILED', f"{url}: {e}")
                
                # 2B: Search for illegal dumping ordinances/fines
                serp_url = "https://serpapi.com/search"
                params = {
                    'q': f'site:.gov "{city_name}" illegal dumping fine ordinance code -site:epa.gov',
                    'api_key': SERPAPI_KEY,
                    'num': 3
                }
                response = requests.get(serp_url, params=params, timeout=15)
                results = response.json()
                
                for result in results.get('organic_results', [])[:2]:
                    url = result['link']
                    is_city_domain = (
                        city_name.lower() in url.lower() or
                        url.startswith('https://dallas.gov') or
                        url.startswith('https://www.dallas.gov') or
                        url.startswith('https://dallascityhall.com')
                    )
                    if not is_city_domain:
                        continue
                    
                    try:
                        if url.lower().endswith('.pdf'):
                            pdf_text = self._extract_pdf(url)
                            if pdf_text:
                                content['pdf_text'].append({'url': url, 'text': pdf_text[:20000]})
                                self._log('ordinance_pdf', 'SUCCESS', url)
                        else:
                            page_response = self.session.get(url, timeout=10, headers={'User-Agent': random.choice(USER_AGENTS)})
                            soup = BeautifulSoup(page_response.content, 'html.parser')
                            for tag in soup(['script', 'style', 'nav', 'footer', 'header']):
                                tag.decompose()
                            text = soup.get_text(separator='\n', strip=True)
                            content['gov_pages'].append({'url': url, 'text': text[:15000]})
                            self._log('ordinance_page', 'SUCCESS', url)
                        time.sleep(random.uniform(1, 3))
                    except Exception as e:
                        self._log('ordinance_page', 'FAILED', f"{url}: {e}")
                
                # 2C: Specific fine amount search (look in snippets for dollar amounts)
                try:
                    serp_url = "https://serpapi.com/search"
                    params = {
                        'q': f'"{city_name}" illegal dumping fine "$" penalty amount ordinance',
                        'api_key': SERPAPI_KEY,
                        'num': 5
                    }
                    response = requests.get(serp_url, params=params, timeout=15)
                    results = response.json()
                    
                    for result in results.get('organic_results', [])[:5]:
                        snippet = result.get('snippet', '')
                        # Look for dollar amounts in snippets directly
                        if any(kw in snippet.lower() for kw in ['fine', 'penalty', 'dumping']):
                            price_match = re.search(r'\$[\d,]+(?:\s*[-‚Äìto]+\s*\$[\d,]+)?', snippet)
                            if price_match:
                                # Extract amount and filter out small fees (parking, permits, etc.)
                                amount_str = price_match.group(0).split()[0].replace('$', '').replace(',', '')
                                try:
                                    amount = int(amount_str)
                                    if amount >= 100:  # Ignore fees under $100 (likely parking/permits)
                                        content['dumping_fine_snippet'] = snippet
                                        content['dumping_fine_raw'] = price_match.group(0)
                                        self._log('fine_search', 'FOUND', price_match.group(0))
                                        break
                                    else:
                                        self._log('fine_search', 'SKIPPED', f"{price_match.group(0)} - too low (likely parking fee)")
                                except ValueError:
                                    # If can't parse, keep it anyway
                                    content['dumping_fine_snippet'] = snippet
                                    content['dumping_fine_raw'] = price_match.group(0)
                                    self._log('fine_search', 'FOUND', price_match.group(0))
                                    break
                except Exception as e:
                    self._log('fine_search', 'ERROR', str(e))
            else:
                # Fallback: Direct URL patterns
                patterns = [
                    f"https://www.{city_name.lower().replace(' ', '')}{state_abbr.lower()}.gov",
                    f"https://{city_name.lower().replace(' ', '')}.{state_abbr.lower()}.gov",
                    f"https://www.ci.{city_name.lower().replace(' ', '-')}.{state_abbr.lower()}.us",
                    f"https://www.{city_name.lower().replace(' ', '')}texas.gov" if state_abbr == 'TX' else None,
                    f"https://www.austintexas.gov" if city_name == 'Austin' else None,
                ]
                
                for url in [p for p in patterns if p]:
                    try:
                        response = self.session.get(url, timeout=10)
                        if response.status_code == 200:
                            soup = BeautifulSoup(response.content, 'html.parser')
                            for tag in soup(['script', 'style', 'nav', 'footer', 'header']):
                                tag.decompose()
                            text = soup.get_text(separator='\n', strip=True)
                            content['gov_pages'].append({'url': url, 'text': text[:15000]})
                            self._log('gov_page_direct', 'SUCCESS', url)
                            
                            # Try to find waste management subpages
                            waste_paths = ['/waste', '/sanitation', '/trash', '/solid-waste', '/bulk-pickup']
                            for path in waste_paths:
                                try:
                                    sub_url = url.rstrip('/') + path
                                    sub_response = self.session.get(sub_url, timeout=10)
                                    if sub_response.status_code == 200:
                                        sub_soup = BeautifulSoup(sub_response.content, 'html.parser')
                                        for tag in sub_soup(['script', 'style', 'nav', 'footer', 'header']):
                                            tag.decompose()
                                        sub_text = sub_soup.get_text(separator='\n', strip=True)
                                        if 'mattress' in sub_text.lower() or 'bulk' in sub_text.lower():
                                            content['gov_pages'].append({'url': sub_url, 'text': sub_text[:15000]})
                                            self._log('gov_page_direct', 'SUCCESS', sub_url)
                                except:
                                    continue
                            break
                    except:
                        continue
        
        except Exception as e:
            self._log('reconnaissance', 'ERROR', str(e))
        
        # Dallas-specific fallback (dallas.gov blocks connections)
        if city_name == 'Dallas' and not content['gov_pages']:
            dallas_fallbacks = [
                'https://dallascityhall.com/departments/sanitation/Pages/brush_and_bulky.aspx',
                'https://dallascityhall.com/departments/sanitation/Pages/default.aspx',
            ]
            for url in dallas_fallbacks:
                try:
                    r = self.session.get(url, timeout=10, headers={'User-Agent': random.choice(USER_AGENTS)})
                    if r.status_code == 200:
                        soup = BeautifulSoup(r.content, 'html.parser')
                        for tag in soup(['script', 'style', 'nav', 'footer', 'header']):
                            tag.decompose()
                        text = soup.get_text(separator='\n', strip=True)
                        content['gov_pages'].append({'url': url, 'text': text[:15000]})
                        self._log('dallas_fallback', 'SUCCESS', url)
                except Exception as e:
                    self._log('dallas_fallback', 'FAILED', str(e))
        
        # 2B: Relevance Router - Filter chunks for mattress/bulk waste relevance
        all_text = "\n\n".join([page['text'] for page in content['gov_pages']] + 
                               [pdf['text'] for pdf in content['pdf_text']])
        
        if all_text:
            content['relevant_chunks'] = await self._filter_relevant_chunks(all_text, city_name)
        
        return content
    
    def _extract_pdf(self, url: str) -> Optional[str]:
        """Extract text from PDF URL using pdfplumber (better than PyPDF2)"""
        try:
            response = self.session.get(url, timeout=30)
            if response.status_code == 200:
                # Save temporarily
                import tempfile
                import os
                with tempfile.NamedTemporaryFile(delete=False, suffix='.pdf') as tmp:
                    tmp.write(response.content)
                    tmp_path = tmp.name
                
                # Try pdfplumber first (best for municipal PDFs)
                try:
                    import pdfplumber
                    text = ""
                    with pdfplumber.open(tmp_path) as pdf:
                        for page in pdf.pages[:20]:  # Limit to first 20 pages
                            page_text = page.extract_text()
                            if page_text:
                                text += page_text + "\n"
                    
                    os.unlink(tmp_path)
                    return text if len(text) > 100 else None
                except ImportError:
                    # Fallback to PyPDF2 if pdfplumber not installed
                    try:
                        from PyPDF2 import PdfReader
                        reader = PdfReader(tmp_path)
                        text = ""
                        for page in reader.pages[:20]:
                            text += page.extract_text() + "\n"
                        
                        os.unlink(tmp_path)
                        return text if len(text) > 100 else None
                    except:
                        os.unlink(tmp_path)
                        return None
                except Exception as e:
                    os.unlink(tmp_path)
                    return None
        except:
            return None
    
    def _repair_json(self, text: str) -> str:
        """Repair common JSON issues from LLM responses"""
        # Remove markdown code blocks
        text = re.sub(r'^```json\s*', '', text, flags=re.MULTILINE)
        text = re.sub(r'^```\s*', '', text, flags=re.MULTILINE)
        text = text.strip()
        
        # Try to extract JSON if embedded in text
        json_match = re.search(r'\{.*\}', text, re.DOTALL)
        if json_match:
            text = json_match.group(0)
        
        # Simple but effective: Replace ALL newlines and tabs with spaces
        # This works because JSON doesn't need newlines - they're just for readability
        text = text.replace('\n', ' ')
        text = text.replace('\r', ' ')
        text = text.replace('\t', ' ')
        
        # Collapse multiple spaces
        text = re.sub(r'\s+', ' ', text)
        
        return text.strip()
    
    async def _filter_relevant_chunks(self, text: str, city_name: str) -> List[str]:
        """
        Relevance Router: Filter text chunks for mattress/bulk waste relevance
        This eliminates Context Collision by removing irrelevant content
        """
        # Split into 500-word chunks
        words = text.split()
        chunks = []
        for i in range(0, len(words), 500):
            chunk = ' '.join(words[i:i+500])
            chunks.append(chunk)
        
        relevant_chunks = []
        
        # Expanded keywords to catch facilities and rules
        keywords = [
            'mattress', 'box spring', 'bulk', 'large item', 'furniture', 'heavy trash',
            'curbside collection', 'drop-off', 'drop off', 'landfill', 'transfer station',
            'recycle center', 'reuse center', 'disposal', 'facility', 'location',
            'hours', 'fee', 'tipping', 'address', 'appointment', 'schedule',
            'illegal dumping', 'fine', 'penalty', 'ordinance', 'code'
        ]
        
        for chunk in chunks[:20]:  # Limit to first 20 chunks
            chunk_lower = chunk.lower()
            if any(kw in chunk_lower for kw in keywords):
                relevant_chunks.append(chunk)
        
        # If we filtered out too much, include more chunks
        if len(relevant_chunks) < 3 and len(chunks) > 0:
            relevant_chunks = chunks[:5]  # Take first 5 chunks as fallback
        
        self._log('relevance_filter', 'SUCCESS', f"Filtered {len(relevant_chunks)}/{len(chunks[:20])} relevant chunks")
        
        return relevant_chunks
    
    async def _agent_dispatcher(self, text: str, city_name: str) -> Dict:
        """Agent 1: Extract contacts - Using plain JSON mode (no Pydantic)"""
        prompt = f"""Extract contact information for {city_name} waste management department.

TEXT:
{text[:3000]}

Find:
- official_phone: The main phone number for waste/sanitation department
- department_name: Official name of the department
- website_url: Official website URL

Return JSON with this exact structure:
{{
  "official_phone": "exact text or null",
  "department_name": "exact text or null",
  "website_url": "exact text or null"
}}

Extract exact text. Use null if not found. No markdown."""
        
        for attempt in range(3):
            try:
                response = self.gemini_model.generate_content(
                    prompt,
                    generation_config=genai.GenerationConfig(
                        response_mime_type="application/json",
                        temperature=0.1
                    )
                )
                result = json.loads(response.text)
                self._log('agent_dispatcher', 'SUCCESS', f"Extracted contacts (attempt {attempt + 1})")
                return result
            except Exception as e:
                if attempt < 2:
                    await asyncio.sleep(2 ** attempt)
                    self._log('agent_dispatcher', 'RETRY', f"Attempt {attempt + 1} failed: {e}")
                else:
                    self._log('agent_dispatcher', 'ERROR', str(e))
                    return {"official_phone": None, "department_name": None, "website_url": None}
    
    async def _agent_rule_enforcer(self, text: str, city_name: str, fine_hint: str = "") -> Dict:
        """Agent 2: Extract rules and fines - Using plain JSON (Gemini has issues with nested schemas)"""
        prompt = f"""Extract curbside collection rules and illegal dumping fines for {city_name}.

TEXT:
{text[:5000]}

CURBSIDE RULES - Find rules for: mattresses, box springs, furniture, bulk items, large items, heavy trash

PRIORITY ORDER for mattress_specific_rule:
1. Look for explicit "mattress" or "box spring" mention ‚Üí use exact quote
2. If not found, use "bulk item", "large item", "furniture", "heavy trash" rules ‚Üí use exact quote
3. If neither, use null

ILLEGAL DUMPING - Find fine amounts and citation references.

{fine_hint}

Return JSON with this structure:
{{
  "curbside_rules": {{
    "is_available": true/false,
    "mattress_specific_rule": "exact text or null",
    "placement_time": "exact text or null",
    "size_limits": "exact text or null",
    "the_catch": "exact text or null",
    "schedule_logic": "exact text or null"
  }},
  "illegal_dumping": {{
    "fine_amount": "exact text or null",
    "citation": "exact text or null"
  }}
}}

Extract exact text. Use null if not found. ONLY extract mattress/furniture/bulk rules, NOT yard waste."""
        
        for attempt in range(3):
            try:
                response = self.gemini_model.generate_content(
                    prompt,
                    generation_config=genai.GenerationConfig(
                        response_mime_type="application/json",
                        temperature=0.1
                    )
                )
                result = json.loads(response.text)
                self._log('agent_rule_enforcer', 'SUCCESS', f"Extracted rules (attempt {attempt + 1})")
                return result
            except Exception as e:
                if attempt < 2:
                    await asyncio.sleep(2 ** attempt)
                    self._log('agent_rule_enforcer', 'RETRY', f"Attempt {attempt + 1} failed: {e}")
                else:
                    self._log('agent_rule_enforcer', 'ERROR', str(e))
                    return {
                        "curbside_rules": {
                            "is_available": False,
                            "mattress_specific_rule": None,
                            "placement_time": None,
                            "size_limits": None,
                            "the_catch": None,
                            "schedule_logic": None
                        },
                        "illegal_dumping": {
                            "fine_amount": None,
                            "citation": None
                        }
                    }
    
    async def _agent_navigator(self, text: str, city_name: str, state_abbr: str) -> List[Dict]:
        """Agent 3: Extract facilities - Using plain JSON mode"""
        prompt = f"""Extract drop-off facilities that accept mattresses/bulk waste in {city_name}, {state_abbr}.

TEXT:
{text[:5000]}

Find facilities that accept: mattresses, box springs, furniture, bulk waste, large items

CRITICAL RULES:
- INCLUDE facilities that accept mattresses/bulk waste (even if they also accept recycling)
- ONLY EXCLUDE facilities that ONLY accept cardboard/bottles/cans/paper
- Extract: name, address, type, hours, tipping_fee, residency_required, notes

EXAMPLES:
‚úÖ INCLUDE: "Recycle & Reuse Drop-Off Center" (accepts mattresses + recycling)
‚úÖ INCLUDE: "Landfill" or "Transfer Station"
‚ùå EXCLUDE: "Recycling Center" (ONLY bottles/cans)

Return JSON with this structure:
{{
  "drop_off_locations": [
    {{
      "name": "exact text",
      "address": "exact text or null",
      "type": "Heavy Trash/Landfill/Transfer Station or null",
      "hours": "exact text or null",
      "tipping_fee": "exact text or null",
      "residency_required": true/false/null,
      "notes": "exact text or null"
    }}
  ]
}}

Extract exact text. Use null if not found. No markdown."""
        
        for attempt in range(3):
            try:
                response = self.gemini_model.generate_content(
                    prompt,
                    generation_config=genai.GenerationConfig(
                        response_mime_type="application/json",
                        temperature=0.1
                    )
                )
                result = json.loads(response.text)
                locations = result.get('drop_off_locations', [])
                self._log('agent_navigator', 'SUCCESS', f"Extracted {len(locations)} facilities (attempt {attempt + 1})")
                return locations
            except Exception as e:
                if attempt < 2:
                    await asyncio.sleep(2 ** attempt)
                    self._log('agent_navigator', 'RETRY', f"Attempt {attempt + 1} failed: {e}")
                else:
                    self._log('agent_navigator', 'ERROR', str(e))
                    return []
    
    async def _agent_auditor(self, extracted: Dict, text: str, city_name: str) -> Dict:
        """Agent 4: Verify and correct extractions - Using plain JSON mode"""
        prompt = f"""Verify and correct this extracted data for {city_name}.

EXTRACTED DATA:
{json.dumps(extracted, indent=2)[:2000]}

SOURCE TEXT:
{text[:2000]}

VERIFICATION RULES:
1. Facilities: KEEP if they accept ANY of: mattresses, bulk waste, furniture, large items
   ONLY REMOVE if facility explicitly accepts ONLY cardboard/glass/cans/paper
2. Rules: If rules mention bulk items or furniture, KEEP even if "mattress" not explicitly mentioned
3. Phone: KEEP if it's for waste/sanitation management

When in doubt, KEEP the data.

Return corrected JSON with same structure. No markdown."""
        
        try:
            response = self.gemini_model.generate_content(
                prompt,
                generation_config=genai.GenerationConfig(
                    response_mime_type="application/json",
                    temperature=0.1
                )
            )
            result = json.loads(response.text)
            self._log('agent_auditor', 'SUCCESS', f"Verified and corrected extractions")
            return result
        except Exception as e:
            self._log('agent_auditor', 'ERROR', str(e))
            return extracted
    
    def _validate_facilities(self, facilities: List[Dict], city_name: str) -> List[Dict]:
        """
        Hard filter: remove any facility that fails basic sanity checks.
        This prevents hallucinated/placeholder entries from reaching the output.
        """
        valid = []
        generic_names = {'landfill', 'transfer station', 'recycling center', 'drop-off center', 'facility'}
        
        for f in facilities:
            name = (f.get('name') or '').strip()
            address = (f.get('address') or '').strip()
            
            # Rule 1: Must have a real name (not just a category label)
            if name.lower() in generic_names:
                self._log('facility_validation', 'REJECTED', f"Generic name with no identity: '{name}'")
                continue
            
            # Rule 2: If it came from LLM (not google_places), must have an address
            if f.get('source') != 'google_places' and not address:
                self._log('facility_validation', 'REJECTED', f"LLM-sourced facility has no address: '{name}'")
                continue
            
            # Rule 3: Google Places facilities must contain city or state in address
            if f.get('source') == 'google_places' and address:
                if city_name.lower() not in address.lower() and 'TX' not in address and 'CA' not in address:
                    self._log('facility_validation', 'REJECTED', f"Address not in target area: '{address}'")
                    continue
            
            valid.append(f)
        
        self._log('facility_validation', 'PASSED', f"{len(valid)}/{len(facilities)} facilities kept")
        return valid
    
    def _apply_known_data_patch(self, extracted: Dict, city_name: str) -> Dict:
        """
        Patches null fields with verified known data.
        Never overwrites a value the LLM successfully extracted.
        """
        known = CITY_KNOWN_DATA.get(city_name, {})
        if not known:
            return extracted
        
        contacts = extracted.setdefault('contacts', {})
        dumping = extracted.setdefault('illegal_dumping', {})
        
        # Only fill nulls ‚Äî never overwrite real extractions
        if not contacts.get('official_phone') and known.get('official_phone'):
            contacts['official_phone'] = known['official_phone']
            self._log('known_data_patch', 'APPLIED', f"phone: {known['official_phone']}")
        
        if not contacts.get('department_name') and known.get('department_name'):
            contacts['department_name'] = known['department_name']
        
        if not contacts.get('website_url') and known.get('website_url'):
            contacts['website_url'] = known['website_url']
            self._log('known_data_patch', 'APPLIED', f"website: {known['website_url']}")
        
        if not dumping.get('fine_amount') and known.get('illegal_dumping_fine'):
            dumping['fine_amount'] = known['illegal_dumping_fine']
            self._log('known_data_patch', 'APPLIED', f"fine: {known['illegal_dumping_fine']}")
        
        if not dumping.get('citation') and known.get('illegal_dumping_citation'):
            dumping['citation'] = known['illegal_dumping_citation']
        
        return extracted
    
    async def _google_places_facilities(self, city_name: str, state_abbr: str) -> List[Dict]:
        """Fallback: Use Google Places API when gov page has no extractable facility data"""
        queries = [
            f"landfill {city_name} {state_abbr}",
            f"transfer station {city_name} {state_abbr}",
            f"bulk waste drop off {city_name} {state_abbr}",
        ]
        
        facilities = []
        seen_names = set()
        
        for query in queries:
            try:
                url = "https://maps.googleapis.com/maps/api/place/textsearch/json"
                params = {
                    'query': query,
                    'key': GOOGLE_MAPS_API_KEY,
                    'type': 'establishment'
                }
                response = requests.get(url, params=params, timeout=10)
                data = response.json()
                
                for place in data.get('results', [])[:2]:
                    name = place.get('name', '')
                    if name in seen_names:
                        continue
                    seen_names.add(name)
                    
                    address = place.get('formatted_address', '')
                    # Only include if it's actually in the target city
                    if city_name.lower() not in address.lower() and state_abbr not in address:
                        continue
                    
                    place_id = place.get('place_id')
                    hours = None
                    
                    # Get hours via Place Details
                    if place_id:
                        detail_url = "https://maps.googleapis.com/maps/api/place/details/json"
                        detail_params = {
                            'place_id': place_id,
                            'fields': 'opening_hours,formatted_phone_number',
                            'key': GOOGLE_MAPS_API_KEY
                        }
                        detail_resp = requests.get(detail_url, params=detail_params, timeout=10)
                        detail_data = detail_resp.json()
                        oh = detail_data.get('result', {}).get('opening_hours', {})
                        if oh.get('weekday_text'):
                            raw_hours = ' | '.join(oh['weekday_text'])
                            hours = raw_hours
                            # Flag suspiciously wide hours
                            if 'Open 24 hours' in raw_hours:
                                f_notes = 'Hours from Google Maps ‚Äî verify before visiting (24h entries may be inaccurate)'
                            else:
                                f_notes = 'Verify accepts mattresses before visiting'
                        else:
                            f_notes = 'Verify accepts mattresses before visiting'
                    
                    facilities.append({
                        'name': name,
                        'address': address,
                        'type': 'Landfill' if 'landfill' in name.lower() else 'Transfer Station' if 'transfer' in name.lower() else 'Drop-Off Center',
                        'hours': hours,
                        'tipping_fee': None,
                        'residency_required': None,
                        'notes': f_notes,
                        'google_maps_url': f"https://www.google.com/maps/place/?q=place_id:{place_id}" if place_id else None,
                        'source': 'google_places'
                    })
                    
                time.sleep(0.5)  # Rate limiting
            except Exception as e:
                self._log('google_places', 'ERROR', str(e))
        
        self._log('google_places_facilities', 'FOUND' if facilities else 'EMPTY', f"{len(facilities)} facilities")
        return facilities
    
    async def _phase3_intelligence(self, content: Dict, city_name: str, state_abbr: str) -> Dict:
        """
        Phase 3: Multi-Agent RAG Pipeline with Granular Schema Splitting
        Uses 3 specialized agents + Skeptical Auditor for 95% accuracy
        """
        relevant_text = "\n\n".join(content.get('relevant_chunks', []))
        
        if not relevant_text:
            # Fallback to full text if no relevant chunks
            relevant_text = "\n\n".join([page['text'] for page in content['gov_pages']][:2])
        
        if not relevant_text:
            self._log('llm_extraction', 'SKIPPED', 'No content to extract from')
            return self._get_fallback_data(city_name, state_abbr)
        
        # Multi-Agent Extraction: 3 specialized agents
        extracted = {}
        
        # Agent 1: The Dispatcher (Contacts & URLs)
        extracted['contacts'] = await self._agent_dispatcher(relevant_text, city_name)
        
        # Agent 2: The Rule Enforcer (Curbside rules & Fines)
        # Pass fine hint if found in search snippets
        fine_hint = ""
        if content.get('dumping_fine_raw'):
            fine_hint = f"\n\nFINE HINT FROM SEARCH: {content['dumping_fine_raw']} - {content.get('dumping_fine_snippet', '')[:200]}"
        
        rules_and_fines = await self._agent_rule_enforcer(relevant_text, city_name, fine_hint)
        
        # Hard fallback: if LLM couldn't find the fine but search did, use search result directly
        if not rules_and_fines.get('illegal_dumping', {}).get('fine_amount'):
            if content.get('dumping_fine_raw'):
                rules_and_fines.setdefault('illegal_dumping', {})
                rules_and_fines['illegal_dumping']['fine_amount'] = content['dumping_fine_raw']
                self._log('fine_fallback', 'APPLIED', f"Using search result: {content['dumping_fine_raw']}")
        
        # Handle None from Optional nested schemas
        if rules_and_fines.get('curbside_rules'):
            extracted['curbside_rules'] = rules_and_fines['curbside_rules']
        else:
            extracted['curbside_rules'] = {
                "is_available": False,
                "mattress_specific_rule": None,
                "placement_time": None,
                "size_limits": None,
                "the_catch": None,
                "schedule_logic": None
            }
        
        if rules_and_fines.get('illegal_dumping'):
            extracted['illegal_dumping'] = rules_and_fines['illegal_dumping']
        else:
            extracted['illegal_dumping'] = {"fine_amount": None, "citation": None}
        
        # Agent 3: The Navigator (Drop-off locations)
        extracted['drop_off_locations'] = await self._agent_navigator(relevant_text, city_name, state_abbr)
        
        # Agent 4: The Skeptical Auditor (Verify extractions)
        # Run auditor always to verify all data
        extracted = await self._agent_auditor(extracted, relevant_text, city_name)
        
        # Ensure illegal_dumping key always exists (auditor can drop it)
        if 'illegal_dumping' not in extracted:
            extracted['illegal_dumping'] = {}
        
        # Apply search-found fine (auditor can't remove what isn't there yet)
        if not extracted['illegal_dumping'].get('fine_amount') and content.get('dumping_fine_raw'):
            extracted['illegal_dumping']['fine_amount'] = content['dumping_fine_raw']
            self._log('fine_fallback', 'APPLIED', content['dumping_fine_raw'])
        
        # If auditor removed all facilities, fall back to Google Places
        if not extracted.get('drop_off_locations'):
            self._log('navigator_fallback', 'INFO', 'Auditor removed facilities, falling back to Google Places API')
            extracted['drop_off_locations'] = await self._google_places_facilities(city_name, state_abbr)
        
        # Validate facilities (remove hallucinated/generic entries)
        extracted['drop_off_locations'] = self._validate_facilities(
            extracted.get('drop_off_locations', []), city_name
        )
        
        # Apply known data (fills anything still null)
        extracted = self._apply_known_data_patch(extracted, city_name)
        
        # Donation policy (simple extraction)
        extracted['donation_policy'] = None  # Can be added later
        
        # Validate extraction quality
        has_data = any([
            extracted.get('contacts', {}).get('official_phone'),
            extracted.get('curbside_rules', {}).get('mattress_specific_rule'),
            len(extracted.get('drop_off_locations', [])) > 0
        ])
        
        if not has_data:
            self._log('llm_extraction', 'WARNING', 'Extracted data is mostly null - may need better source pages')
        else:
            self._log('llm_extraction', 'SUCCESS', f"Multi-agent extraction complete with data")
        
        return extracted
    
    async def _phase4_charisma(self, extracted_data: Dict, geo_data: Dict, city_name: str, state_abbr: str) -> Dict:
        """
        Phase 4: LLM-driven copywriting for SEO and conversion
        Uses Gemini 2.5 Flash with plain JSON mode
        """
        fine_amount = extracted_data.get('illegal_dumping', {}).get('fine_amount', '$500-$2,000')
        
        copywriting_prompt = f"""Generate SEO copy for {city_name}, {state_abbr} mattress disposal guide.

CONTEXT:
- Fine: {fine_amount}
- Weather: {"Rainy" if geo_data.get('weather_profile', {}).get('is_rain_heavy') else "Dry"}
- Curbside: {"Available" if extracted_data.get('curbside_rules', {}).get('is_available', False) else "Not available"}

Create 4 pieces of copy:
- hero_hook: One sentence pain point for {city_name}
- seo_title: Title with 2026 and keywords (under 60 chars)
- seo_description: Description with fine and rules (under 155 chars)
- neighborhoods: 15 comma-separated neighborhood names

EXAMPLES:
Austin: "Don't want to drive to the landfill in 100-degree heat?"
NYC: "Can't carry it down five flights of stairs?"
Seattle: "Don't want to risk a wet mattress rejection?"

NEIGHBORHOODS: "Hyde Park, Zilker, East Austin, South Congress, Barton Hills, Clarksville, Bouldin Creek, Travis Heights, Allandale, Rosedale, Tarrytown, Mueller, Domain, Westlake, Bee Cave"

Return JSON with this structure:
{{
  "hero_hook": "one sentence",
  "seo_title": "title",
  "seo_description": "description",
  "neighborhoods": "15 neighborhoods"
}}

No markdown."""

        for attempt in range(3):
            try:
                response = self.gemini_model.generate_content(
                    copywriting_prompt,
                    generation_config=genai.GenerationConfig(
                        response_mime_type="application/json",
                        temperature=0.7  # Higher for creativity
                    )
                )
                
                charisma = json.loads(response.text)
                self._log('charisma_synthesis', 'SUCCESS', f"Generated copy (attempt {attempt + 1})")
                return charisma
                
            except Exception as e:
                if attempt < 2:
                    await asyncio.sleep(2 ** attempt)
                    self._log('charisma_synthesis', 'RETRY', f"Attempt {attempt + 1} failed: {e}")
                else:
                    self._log('charisma_synthesis', 'ERROR', str(e))
                    return {
                        'hero_hook': f"Need to dispose of a mattress in {city_name}?",
                        'seo_title': f"Mattress Disposal {city_name}: 2026 Guide",
                        'seo_description': f"Official guide to mattress disposal in {city_name}, {state_abbr}. Rules, fees, and private haulers.",
                        'neighborhoods': f"Downtown {city_name}, Midtown, Uptown, East {city_name}, West {city_name}, North {city_name}, South {city_name}, Central {city_name}, {city_name} Heights, Old {city_name}, New {city_name}, {city_name} Park, {city_name} Hills, {city_name} Village, Greater {city_name}"
                    }
    
    async def _phase5_competitor(self, city_name: str, state_abbr: str) -> Dict:
        """
        Phase 5: Competitor price scraping
        Scrapes 1-800-GOT-JUNK and similar competitors for pricing
        """
        competitor_data = {
            'competitor_name': 'National Junk Chains',
            'competitor_price': '$139+',
            'value_prop': 'No franchise fees. Just local haulers.'
        }
        
        try:
            if SERPAPI_KEY:
                # Search for competitor pricing
                serp_url = "https://serpapi.com/search"
                params = {
                    'q': f'1-800-got-junk mattress removal {city_name} {state_abbr} price',
                    'api_key': SERPAPI_KEY,
                    'num': 3
                }
                response = requests.get(serp_url, params=params, timeout=15)
                results = response.json()
                
                # Look for pricing in snippets
                for result in results.get('organic_results', [])[:3]:
                    snippet = result.get('snippet', '')
                    # Extract price patterns
                    price_match = re.search(r'\$\d{2,3}', snippet)
                    if price_match:
                        competitor_data['competitor_price'] = f"{price_match.group(0)}+"
                        self._log('competitor_price', 'EXTRACTED', competitor_data['competitor_price'])
                        break
        
        except Exception as e:
            self._log('competitor_scraping', 'ERROR', str(e))
        
        return competitor_data
    
    def _phase6_assembly(self, city_name: str, state_name: str, state_abbr: str,
                         geo_data: Dict, extracted_data: Dict, charisma_data: Dict,
                         competitor_data: Dict) -> Dict:
        """
        Phase 6: Assemble final v5.0 schema with validation
        """
        # Normalize website URL
        contacts = extracted_data.get('contacts', {}).copy()
        if contacts.get('website_url'):
            url = contacts['website_url']
            if not url.startswith('http'):
                contacts['website_url'] = 'https://www.' + url
        
        # Build complete city object
        city_data = {
            'city_slug': f"{city_name.lower().replace(' ', '-')}-{state_abbr.lower()}",
            'city_name': city_name,
            'state_name': state_name,
            'state_slug': state_name.lower().replace(' ', '-'),
            'state_abbr': state_abbr,
            
            # Phase 1: Geo data
            'geo': {
                'latitude': geo_data.get('latitude'),
                'longitude': geo_data.get('longitude'),
                'zip_codes': geo_data.get('zip_codes', [])
            },
            
            # Phase 4: SEO & Charisma - Always include seo field
            'seo': {
                'title_override': charisma_data.get('seo_title'),
                'meta_desc_override': charisma_data.get('seo_description')
            },
            'hero_hook': charisma_data.get('hero_hook', ''),
            'neighborhoods': charisma_data.get('neighborhoods', ''),
            
            # Phase 1: Population
            'population': geo_data.get('population', {
                'count': None,
                'year': 2020,
                'source': f"https://www.census.gov/quickfacts/{city_name.lower().replace(' ', '')}city{state_abbr.lower()}"
            }),
            
            # Phase 3: Extracted data
            'contacts': contacts,
            'curbside_rules': extracted_data.get('curbside_rules', {}),
            'weather_profile': geo_data.get('weather_profile', {}),
            'drop_off_locations': extracted_data.get('drop_off_locations', []),
            'donation_policy': extracted_data.get('donation_policy'),
            'illegal_dumping': extracted_data.get('illegal_dumping', {}),
            
            # Phase 5: Affiliate & Competitor
            'affiliate_config': {
                'partner_name': 'LoadUp',
                'custom_link_slug': f"{city_name.lower().replace(' ', '-')}-{state_abbr.lower()}-mattress-disposal",
                'base_price_display': '$80',
                'competitor_comparison': competitor_data
            },
            
            # Audit metadata
            'audit_metadata': {
                'confidence_score': self._calculate_confidence(extracted_data, geo_data),
                'verification_checklist': {
                    'gov_source_found': bool(extracted_data.get('contacts', {}).get('website_url')),
                    'mattress_rule_verified': bool(extracted_data.get('curbside_rules', {}).get('mattress_specific_rule')),
                    'facility_hours_verified': len(extracted_data.get('drop_off_locations', [])) > 0,
                    'facility_type_verified': True,
                    'population_census_verified': False  # Needs API call
                },
                'sources_used': [
                    entry['details'] for entry in self.verification_log
                    if entry['status'] == 'SUCCESS' and entry['field'] in
                    ('gov_page_scraped', 'ordinance_page', 'dallas_fallback', 'pdf_extracted')
                ],
                'last_updated': datetime.now().strftime('%Y-%m-%d'),
                'scraping_method': 'AUTONOMOUS_MULTI_AGENT_PIPELINE'
            }
        }
        
        # Add Google Maps URLs to facilities
        for facility in city_data['drop_off_locations']:
            if facility.get('address'):
                # Generate Google Maps URL
                address_encoded = facility['address'].replace(' ', '+')
                facility['google_maps_url'] = f"https://www.google.com/maps/search/?api=1&query={address_encoded}"
        
        return city_data
    
    def _calculate_confidence(self, extracted_data: Dict, geo_data: Dict) -> str:
        """Calculate confidence score based on data completeness"""
        checks = [
            bool(extracted_data.get('contacts', {}).get('official_phone')),
            bool(extracted_data.get('curbside_rules', {}).get('mattress_specific_rule')),
            len(extracted_data.get('drop_off_locations', [])) > 0,
            bool(extracted_data.get('illegal_dumping', {}).get('fine_amount')),
            bool(geo_data.get('latitude')),
            bool(extracted_data.get('contacts', {}).get('website_url'))  # Replaces zip_codes check
        ]
        
        score = sum(checks) / len(checks)
        
        if score >= 0.8:
            return 'HIGH'
        elif score >= 0.5:
            return 'MEDIUM'
        else:
            return 'LOW'
    
    def _get_fallback_data(self, city_name: str, state_abbr: str) -> Dict:
        """Fallback data structure when extraction fails"""
        return {
            'contacts': {
                'official_phone': '3-1-1 (City General Line)',
                'department_name': None,
                'website_url': None
            },
            'curbside_rules': {
                'is_available': False,
                'mattress_specific_rule': None,
                'placement_time': None,
                'size_limits': None,
                'the_catch': None,
                'schedule_logic': None
            },
            'drop_off_locations': [],
            'illegal_dumping': {
                'fine_amount': None,
                'citation': None
            },
            'donation_policy': None
        }
    
    def _log(self, field: str, status: str, details: str):
        """Log verification steps"""
        log_entry = {
            'field': field,
            'status': status,
            'details': details[:200],
            'timestamp': datetime.now().isoformat()
        }
        self.verification_log.append(log_entry)
        print(f"  [{status}] {field}: {details[:100]}")


async def main():
    """
    Test the autonomous scraper
    """
    scraper = AutonomousScraper()
    
    # Test cities
    test_cities = [
        ('Austin', 'TX', 'Texas'),
        ('Dallas', 'TX', 'Texas'),
        ('Houston', 'TX', 'Texas')
    ]
    
    results = []
    
    for city_name, state_abbr, state_name in test_cities:
        try:
            result = await scraper.scrape_city_autonomous(city_name, state_abbr, state_name)
            results.append(result)
            
            # Save individual city - detect if running from project root or scripts/
            import os
            if os.path.exists('data'):
                output_file = f"data/autonomous_{city_name.lower()}.json"
            else:
                output_file = f"../data/autonomous_{city_name.lower()}.json"
            
            with open(output_file, 'w') as f:
                json.dump(result, f, indent=2)
            
            print(f"\n‚úÖ Saved: {output_file}")
            print(f"Confidence: {result['audit_metadata']['confidence_score']}")
            
            # Rate limiting
            await asyncio.sleep(random.uniform(3, 7))
            
        except Exception as e:
            print(f"\n‚ùå ERROR processing {city_name}: {e}")
            import traceback
            traceback.print_exc()
            continue
    
    # Save all results - detect path
    if os.path.exists('data'):
        output_file = 'data/autonomous_cities.json'
    else:
        output_file = '../data/autonomous_cities.json'
    
    with open(output_file, 'w') as f:
        json.dump(results, f, indent=2)
    
    print(f"\n{'='*80}")
    print(f"‚úÖ PIPELINE COMPLETE: {len(results)} cities processed")
    print(f"{'='*80}")


if __name__ == '__main__':
    asyncio.run(main())
